{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorlayer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1cnwSCfGYW6",
        "outputId": "26e50b52-63a1-4cc7-edb5-4d43bb5e7d34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorlayer\n",
            "  Downloading tensorlayer-2.2.5-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: imageio>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorlayer) (2.37.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.12/dist-packages (from tensorlayer) (2.0.2)\n",
            "Requirement already satisfied: progressbar2>=3.39.3 in /usr/local/lib/python3.12/dist-packages (from tensorlayer) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorlayer) (2.32.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorlayer) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from tensorlayer) (0.25.2)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorlayer) (1.16.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from tensorlayer) (2.0.0)\n",
            "Requirement already satisfied: h5py>=2.9 in /usr/local/lib/python3.12/dist-packages (from tensorlayer) (3.15.1)\n",
            "Requirement already satisfied: cloudpickle>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from tensorlayer) (3.1.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio>=2.5.0->tensorlayer) (11.3.0)\n",
            "Requirement already satisfied: python-utils>=3.8.1 in /usr/local/lib/python3.12/dist-packages (from progressbar2>=3.39.3->tensorlayer) (3.9.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->tensorlayer) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->tensorlayer) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->tensorlayer) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->tensorlayer) (2025.10.5)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.15.0->tensorlayer) (3.5)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.15.0->tensorlayer) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.15.0->tensorlayer) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.15.0->tensorlayer) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.0->tensorlayer) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.0->tensorlayer) (3.6.0)\n",
            "Requirement already satisfied: typing_extensions>3.10.0.2 in /usr/local/lib/python3.12/dist-packages (from python-utils>=3.8.1->progressbar2>=3.39.3->tensorlayer) (4.15.0)\n",
            "Downloading tensorlayer-2.2.5-py3-none-any.whl (381 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m381.2/381.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorlayer\n",
            "Successfully installed tensorlayer-2.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "from tensorlayer.layers import Dense, Dropout, Flatten, Input\n",
        "from tensorlayer.models import Model\n",
        "\n",
        "num_classes = 10\n",
        "epochs = 20\n",
        "batch_size = 128\n",
        "input_shape = (None, 784)\n",
        "\n",
        "def contrastive_loss(label, feature1, feature2):\n",
        "    margin = 1.0\n",
        "    eucd = tf.sqrt(tf.reduce_sum(tf.square(feature1 - feature2), axis=1))\n",
        "    return tf.reduce_mean(label * tf.square(eucd) + (1 - label) * tf.square(tf.maximum(margin - eucd, 0)))\n",
        "\n",
        "def compute_accuracy(label, feature1, feature2):\n",
        "    eucd = tf.sqrt(tf.reduce_sum((feature1 - feature2)**2, axis=1))\n",
        "    pred = tf.cast(eucd < 0.5, label.dtype)\n",
        "    return tf.reduce_mean(tf.cast(tf.equal(pred, label), tf.float32))\n",
        "\n",
        "def create_base_network(input_shape):\n",
        "    input = Input(shape=input_shape)\n",
        "    x = Flatten()(input)\n",
        "    x = Dense(128, act=tf.nn.relu)(x)\n",
        "    x = Dropout(0.9)(x)\n",
        "    x = Dense(128, act=tf.nn.relu)(x)\n",
        "    x = Dropout(0.9)(x)\n",
        "    x = Dense(128, act=tf.nn.relu)(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "def get_siamese_network(input_shape):\n",
        "    base_layer = create_base_network(input_shape).as_layer()\n",
        "    ni_1 = Input(input_shape)\n",
        "    ni_2 = Input(input_shape)\n",
        "    nn_1 = base_layer(ni_1)\n",
        "    nn_2 = base_layer(ni_2)\n",
        "    return Model(inputs=[ni_1, ni_2], outputs=[nn_1, nn_2])\n",
        "\n",
        "def create_pairs(x, digit_indices):\n",
        "    pairs = []\n",
        "    labels = []\n",
        "    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\n",
        "    for d in range(num_classes):\n",
        "        for i in range(n):\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
        "            pairs += [[x[z1], x[z2]]]\n",
        "            inc = random.randrange(1, num_classes)\n",
        "            dn = (d + inc) % num_classes\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
        "            pairs += [[x[z1], x[z2]]]\n",
        "            labels += [1, 0]\n",
        "    return np.array(pairs), np.array(labels).astype(np.float32)\n",
        "\n",
        "try:\n",
        "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    X_train = X_train.reshape(-1, 784).astype(np.float32) / 255.0\n",
        "    X_test = X_test.reshape(-1, 784).astype(np.float32) / 255.0\n",
        "    y_train = y_train.astype(np.int64)\n",
        "    y_test = y_test.astype(np.int64)\n",
        "\n",
        "    X_val = X_train[50000:]\n",
        "    y_val = y_train[50000:]\n",
        "    X_train = X_train[:50000]\n",
        "    y_train = y_train[:50000]\n",
        "    tl.logging.info(\"Data loaded and preprocessed successfully using tf.keras.\")\n",
        "except Exception as e:\n",
        "    tl.logging.error(f\"Failed to load data using tf.keras: {e}\")\n",
        "    exit()\n",
        "\n",
        "digit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\n",
        "tr_pairs, tr_y = create_pairs(X_train, digit_indices)\n",
        "digit_indices = [np.where(y_val == i)[0] for i in range(num_classes)]\n",
        "val_pairs, val_y = create_pairs(X_val, digit_indices)\n",
        "digit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]\n",
        "te_pairs, te_y = create_pairs(X_test, digit_indices)\n",
        "\n",
        "model = get_siamese_network(input_shape)\n",
        "train_weights = model.trainable_weights\n",
        "optimizer = tf.optimizers.RMSprop()\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/siamese/' + current_time + '/train'\n",
        "val_log_dir = 'logs/siamese/' + current_time + '/validation'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n",
        "tl.logging.info(f\"TensorBoard logs will be saved to: logs/siamese/{current_time}\")\n",
        "\n",
        "print_freq = 5\n",
        "\n",
        "@tf.function\n",
        "def train_step(X_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        _out1, _out2 = model([X_batch[:, 0, :], X_batch[:, 1, :]])\n",
        "        _loss = contrastive_loss(y_batch, _out1, _out2)\n",
        "    grad = tape.gradient(_loss, train_weights)\n",
        "    optimizer.apply_gradients(zip(grad, train_weights))\n",
        "    _acc = compute_accuracy(y_batch, _out1, _out2)\n",
        "    return _loss, _acc\n",
        "\n",
        "@tf.function\n",
        "def eval_step(X_batch, y_batch):\n",
        "    _out1, _out2 = model([X_batch[:, 0, :], X_batch[:, 1, :]])\n",
        "    _loss = contrastive_loss(y_batch, _out1, _out2)\n",
        "    _acc = compute_accuracy(y_batch, _out1, _out2)\n",
        "    return _loss, _acc\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc, n_iter = 0, 0, 0\n",
        "    model.train()\n",
        "    for X_batch, y_batch in tl.iterate.minibatches(tr_pairs, tr_y, batch_size, shuffle=True):\n",
        "        _loss, _acc = train_step(X_batch, y_batch)\n",
        "        train_loss += _loss\n",
        "        train_acc += _acc\n",
        "        n_iter += 1\n",
        "    avg_train_loss = train_loss / n_iter\n",
        "    avg_train_acc = train_acc / n_iter\n",
        "\n",
        "    with train_summary_writer.as_default():\n",
        "        tf.summary.scalar('loss', avg_train_loss, step=epoch)\n",
        "        tf.summary.scalar('accuracy', avg_train_acc, step=epoch)\n",
        "\n",
        "    val_loss, val_acc, n_iter_val = 0, 0, 0\n",
        "    model.eval()\n",
        "    for X_batch, y_batch in tl.iterate.minibatches(val_pairs, val_y, batch_size, shuffle=False):\n",
        "        _loss, _acc = eval_step(X_batch, y_batch)\n",
        "        val_loss += _loss\n",
        "        val_acc += _acc\n",
        "        n_iter_val += 1\n",
        "    avg_val_loss = val_loss / n_iter_val\n",
        "    avg_val_acc = val_acc / n_iter_val\n",
        "\n",
        "    with val_summary_writer.as_default():\n",
        "        tf.summary.scalar('loss', avg_val_loss, step=epoch)\n",
        "        tf.summary.scalar('accuracy', avg_val_acc, step=epoch)\n",
        "\n",
        "    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} took {time.time() - start_time:.2f}s\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f}\")\n",
        "        print(f\"  Val Loss:   {avg_val_loss:.4f}, Val Acc:   {avg_val_acc:.4f}\")\n",
        "\n",
        "# --- Ø¨Ø®Ø´ Û¶: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª ---\n",
        "print(\"\\nStarting final test evaluation...\")\n",
        "model.eval()\n",
        "test_loss, test_acc, n_iter_test = 0, 0, 0\n",
        "for X_batch, y_batch in tl.iterate.minibatches(te_pairs, te_y, batch_size, shuffle=False):\n",
        "    _loss, _acc = eval_step(X_batch, y_batch)\n",
        "    test_loss += _loss\n",
        "    test_acc += _acc\n",
        "    n_iter_test += 1\n",
        "print(f\"  Test Loss: {test_loss / n_iter_test:.4f}\")\n",
        "print(f\"  Test Acc:  {test_acc / n_iter_test:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wROVph4LC21",
        "outputId": "da8a1866-0d8a-46ae-e215-1f03b790bdf6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 took 11.35s\n",
            "  Train Loss: 0.0820, Train Acc: 0.9070\n",
            "  Val Loss:   0.0381, Val Acc:   0.9575\n",
            "Epoch 5/20 took 7.41s\n",
            "  Train Loss: 0.0171, Train Acc: 0.9828\n",
            "  Val Loss:   0.0233, Val Acc:   0.9728\n",
            "Epoch 10/20 took 7.57s\n",
            "  Train Loss: 0.0102, Train Acc: 0.9895\n",
            "  Val Loss:   0.0233, Val Acc:   0.9732\n",
            "Epoch 15/20 took 6.12s\n",
            "  Train Loss: 0.0078, Train Acc: 0.9922\n",
            "  Val Loss:   0.0228, Val Acc:   0.9735\n",
            "Epoch 20/20 took 6.67s\n",
            "  Train Loss: 0.0062, Train Acc: 0.9938\n",
            "  Val Loss:   0.0245, Val Acc:   0.9731\n",
            "\n",
            "Starting final test evaluation...\n",
            "  Test Loss: 0.0255\n",
            "  Test Acc:  0.9720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "from tensorlayer.layers import Dense, Dropout, Flatten, Input\n",
        "from tensorlayer.models import Model\n",
        "\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "EXPERIMENT_CONFIGS = [\n",
        "    {'LR': 0.001, 'DROPOUT': 0.3, 'EMBEDDING': 128, 'MARGIN': 1.0},\n",
        "    {'LR': 0.0001, 'DROPOUT': 0.3, 'EMBEDDING': 128, 'MARGIN': 1.0},\n",
        "    {'LR': 0.001, 'DROPOUT': 0.5, 'EMBEDDING': 128, 'MARGIN': 1.0},\n",
        "    {'LR': 0.001, 'DROPOUT': 0.3, 'EMBEDDING': 64, 'MARGIN': 1.0},\n",
        "    {'LR': 0.001, 'DROPOUT': 0.3, 'EMBEDDING': 128, 'MARGIN': 1.5},\n",
        "]\n",
        "\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 128\n",
        "NUM_CLASSES = 10\n",
        "INPUT_SHAPE = (None, 784)\n",
        "PRINT_FREQ = 5\n",
        "\n",
        "# ==============================================================================\n",
        "# ==============================================================================\n",
        "\n",
        "def contrastive_loss(label, feature1, feature2, margin):\n",
        "    eucd = tf.sqrt(tf.reduce_sum(tf.square(feature1 - feature2), axis=1))\n",
        "    return tf.reduce_mean(label * tf.square(eucd) + (1 - label) * tf.square(tf.maximum(margin - eucd, 0)))\n",
        "\n",
        "def compute_accuracy(label, feature1, feature2, margin):\n",
        "    eucd = tf.sqrt(tf.reduce_sum((feature1 - feature2)**2, axis=1))\n",
        "    pred = tf.cast(eucd < (margin / 2.0), label.dtype)\n",
        "    return tf.reduce_mean(tf.cast(tf.equal(pred, label), tf.float32))\n",
        "\n",
        "# --- Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø´Ø¨Ú©Ù‡ ---\n",
        "def create_base_network(input_shape, embedding_units, dropout_rate):\n",
        "    input = Input(shape=input_shape)\n",
        "    x = Flatten()(input)\n",
        "    x = Dense(embedding_units, act=tf.nn.relu)(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    x = Dense(embedding_units, act=tf.nn.relu)(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    x = Dense(embedding_units, act=tf.nn.relu)(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "def get_siamese_network(input_shape, embedding_units, dropout_rate):\n",
        "    base_layer = create_base_network(input_shape, embedding_units, dropout_rate).as_layer()\n",
        "    ni_1 = Input(input_shape)\n",
        "    ni_2 = Input(input_shape)\n",
        "    nn_1 = base_layer(ni_1)\n",
        "    nn_2 = base_layer(ni_2)\n",
        "    return Model(inputs=[ni_1, ni_2], outputs=[nn_1, nn_2])\n",
        "\n",
        "def create_pairs(x, digit_indices):\n",
        "    pairs = []\n",
        "    labels = []\n",
        "    n = min([len(digit_indices[d]) for d in range(NUM_CLASSES)]) - 1\n",
        "    for d in range(NUM_CLASSES):\n",
        "        for i in range(n):\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
        "            pairs += [[x[z1], x[z2]]]\n",
        "            inc = random.randrange(1, NUM_CLASSES)\n",
        "            dn = (d + inc) % NUM_CLASSES\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
        "            pairs += [[x[z1], x[z2]]]\n",
        "            labels += [1, 0]\n",
        "    return np.array(pairs), np.array(labels).astype(np.float32)\n",
        "\n",
        "# ==============================================================================\n",
        "# ==============================================================================\n",
        "def load_and_prep_data():\n",
        "    tl.logging.info(\"Loading and preprocessing data... (This happens only once)\")\n",
        "    try:\n",
        "        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "        X_train = X_train.reshape(-1, 784).astype(np.float32) / 255.0\n",
        "        X_test = X_test.reshape(-1, 784).astype(np.float32) / 255.0\n",
        "        y_train = y_train.astype(np.int64)\n",
        "        y_test = y_test.astype(np.int64)\n",
        "\n",
        "        X_val = X_train[50000:]\n",
        "        y_val = y_train[50000:]\n",
        "        X_train = X_train[:50000]\n",
        "        y_train = y_train[:50000]\n",
        "    except Exception as e:\n",
        "        tl.logging.error(f\"Failed to load data: {e}\")\n",
        "        return None\n",
        "\n",
        "    digit_indices = [np.where(y_train == i)[0] for i in range(NUM_CLASSES)]\n",
        "    tr_pairs, tr_y = create_pairs(X_train, digit_indices)\n",
        "    digit_indices = [np.where(y_val == i)[0] for i in range(NUM_CLASSES)]\n",
        "    val_pairs, val_y = create_pairs(X_val, digit_indices)\n",
        "    digit_indices = [np.where(y_test == i)[0] for i in range(NUM_CLASSES)]\n",
        "    te_pairs, te_y = create_pairs(X_test, digit_indices)\n",
        "\n",
        "    return (tr_pairs, tr_y), (val_pairs, val_y), (te_pairs, te_y)\n",
        "\n",
        "# ==============================================================================\n",
        "# ==============================================================================\n",
        "def run_experiment(params, prepared_data):\n",
        "    LR = params['LR']\n",
        "    DROPOUT = params['DROPOUT']\n",
        "    EMBEDDING = params['EMBEDDING']\n",
        "    MARGIN = params['MARGIN']\n",
        "\n",
        "    (tr_pairs, tr_y), (val_pairs, val_y), (te_pairs, te_y) = prepared_data\n",
        "\n",
        "    model = get_siamese_network(INPUT_SHAPE, EMBEDDING, DROPOUT)\n",
        "    train_weights = model.trainable_weights\n",
        "    optimizer = tf.optimizers.RMSprop(learning_rate=LR)\n",
        "\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    run_name = f\"LR={LR}_DR={DROPOUT}_E={EMBEDDING}_M={MARGIN}\"\n",
        "    log_dir = f'logs/siamese/{current_time}_{run_name}'\n",
        "    train_summary_writer = tf.summary.create_file_writer(log_dir + '/train')\n",
        "    val_summary_writer = tf.summary.create_file_writer(log_dir + '/validation')\n",
        "    tl.logging.info(f\"TensorBoard log for this run: {log_dir}\")\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(X_batch, y_batch):\n",
        "        with tf.GradientTape() as tape:\n",
        "            _out1, _out2 = model([X_batch[:, 0, :], X_batch[:, 1, :]])\n",
        "            _loss = contrastive_loss(y_batch, _out1, _out2, margin=MARGIN)\n",
        "        grad = tape.gradient(_loss, train_weights)\n",
        "        optimizer.apply_gradients(zip(grad, train_weights))\n",
        "        _acc = compute_accuracy(y_batch, _out1, _out2, margin=MARGIN)\n",
        "        return _loss, _acc\n",
        "\n",
        "    @tf.function\n",
        "    def eval_step(X_batch, y_batch):\n",
        "        _out1, _out2 = model([X_batch[:, 0, :], X_batch[:, 1, :]])\n",
        "        _loss = contrastive_loss(y_batch, _out1, _out2, margin=MARGIN)\n",
        "        _acc = compute_accuracy(y_batch, _out1, _out2, margin=MARGIN)\n",
        "        return _loss, _acc\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(EPOCHS):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc, n_iter = 0, 0, 0\n",
        "        model.train()\n",
        "        for X_batch, y_batch in tl.iterate.minibatches(tr_pairs, tr_y, BATCH_SIZE, shuffle=True):\n",
        "            _loss, _acc = train_step(X_batch, y_batch)\n",
        "            train_loss += _loss\n",
        "            train_acc += _acc\n",
        "            n_iter += 1\n",
        "        avg_train_loss = train_loss / n_iter\n",
        "        avg_train_acc = train_acc / n_iter\n",
        "\n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.scalar('loss', avg_train_loss, step=epoch)\n",
        "            tf.summary.scalar('accuracy', avg_train_acc, step=epoch)\n",
        "\n",
        "        val_loss, val_acc, n_iter_val = 0, 0, 0\n",
        "        model.eval()\n",
        "        for X_batch, y_batch in tl.iterate.minibatches(val_pairs, val_y, BATCH_SIZE, shuffle=False):\n",
        "            _loss, _acc = eval_step(X_batch, y_batch)\n",
        "            val_loss += _loss\n",
        "            val_acc += _acc\n",
        "            n_iter_val += 1\n",
        "        avg_val_loss = val_loss / n_iter_val\n",
        "        avg_val_acc = val_acc / n_iter_val\n",
        "\n",
        "        if avg_val_acc > best_val_acc:\n",
        "            best_val_acc = avg_val_acc\n",
        "\n",
        "        with val_summary_writer.as_default():\n",
        "            tf.summary.scalar('loss', avg_val_loss, step=epoch)\n",
        "            tf.summary.scalar('accuracy', avg_val_acc, step=epoch)\n",
        "\n",
        "        if epoch + 1 == 1 or (epoch + 1) % PRINT_FREQ == 0:\n",
        "            print(f\"  Epoch {epoch + 1}/{EPOCHS} - Train Acc: {avg_train_acc:.4f} - Val Acc: {avg_val_acc:.4f}\")\n",
        "\n",
        "    test_loss, test_acc, n_iter_test = 0, 0, 0\n",
        "    model.eval()\n",
        "    for X_batch, y_batch in tl.iterate.minibatches(te_pairs, te_y, BATCH_SIZE, shuffle=False):\n",
        "        _loss, _acc = eval_step(X_batch, y_batch)\n",
        "        test_loss += _loss\n",
        "        test_acc += _acc\n",
        "        n_iter_test += 1\n",
        "\n",
        "    final_test_loss = test_loss / n_iter_test\n",
        "    final_test_acc = test_acc / n_iter_test\n",
        "\n",
        "    print(f\"  Final Test Acc: {final_test_acc:.4f}, Best Val Acc: {best_val_acc:.4f}\")\n",
        "\n",
        "    return final_test_loss, final_test_acc, best_val_acc\n",
        "\n",
        "# ==============================================================================\n",
        "# ==============================================================================\n",
        "def print_results_table(results):\n",
        "    print(\"\\n\\n\" + \"=\"*80)\n",
        "    print(\"ğŸ FINAL EXPERIMENT RESULTS ğŸ\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    header = \"| # | LR     | Dropout | Embedding | Margin | Test Acc | Test Loss | Best Val Acc |\"\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "\n",
        "    sorted_results = sorted(results, key=lambda x: x['test_acc'], reverse=True)\n",
        "\n",
        "    for i, res in enumerate(sorted_results):\n",
        "        p = res['params']\n",
        "        print(\n",
        "            f\"| {i+1:<1} \"\n",
        "            f\"| {p['LR']:<6} \"\n",
        "            f\"| {p['DROPOUT']:<7} \"\n",
        "            f\"| {p['EMBEDDING']:<9} \"\n",
        "            f\"| {p['MARGIN']:<6} \"\n",
        "            f\"| {res['test_acc'] * 100:6.2f}%   \"  # Ù†Ù…Ø§ÛŒØ´ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø±ØµØ¯\n",
        "            f\"| {res['test_loss']:9.4f} \"\n",
        "            f\"| {res['val_acc'] * 100:10.2f}%   |\"\n",
        "        )\n",
        "    print(\"-\" * len(header))\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    prepared_data = load_and_prep_data()\n",
        "    if prepared_data is None:\n",
        "        print(\"Failed to load data, exiting.\")\n",
        "        exit()\n",
        "\n",
        "    final_results = []\n",
        "\n",
        "    for i, params in enumerate(EXPERIMENT_CONFIGS):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"ğŸš€ STARTING EXPERIMENT {i+1}/{len(EXPERIMENT_CONFIGS)}\")\n",
        "        print(f\"Parameters: {params}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        test_loss, test_acc, val_acc = run_experiment(params, prepared_data)\n",
        "\n",
        "        result_data = {\n",
        "            'params': params,\n",
        "            'test_loss': float(test_loss),\n",
        "            'test_acc': float(test_acc),\n",
        "            'val_acc': float(val_acc)\n",
        "        }\n",
        "        final_results.append(result_data)\n",
        "\n",
        "    print_results_table(final_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUfIfpAvOUxV",
        "outputId": "eaae9be4-7649-42a1-c297-68d4d8fc733e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ğŸš€ STARTING EXPERIMENT 1/5\n",
            "Parameters: {'LR': 0.001, 'DROPOUT': 0.3, 'EMBEDDING': 128, 'MARGIN': 1.0}\n",
            "==================================================\n",
            "  Epoch 1/20 - Train Acc: 0.5147 - Val Acc: 0.6571\n",
            "  Epoch 5/20 - Train Acc: 0.8178 - Val Acc: 0.8555\n",
            "  Epoch 10/20 - Train Acc: 0.8388 - Val Acc: 0.8846\n",
            "  Epoch 15/20 - Train Acc: 0.6510 - Val Acc: 0.5000\n",
            "  Epoch 20/20 - Train Acc: 0.4999 - Val Acc: 0.5000\n",
            "  Final Test Acc: 0.5000, Best Val Acc: 0.8894\n",
            "\n",
            "==================================================\n",
            "ğŸš€ STARTING EXPERIMENT 2/5\n",
            "Parameters: {'LR': 0.0001, 'DROPOUT': 0.3, 'EMBEDDING': 128, 'MARGIN': 1.0}\n",
            "==================================================\n",
            "  Epoch 1/20 - Train Acc: 0.5021 - Val Acc: 0.5000\n",
            "  Epoch 5/20 - Train Acc: 0.5109 - Val Acc: 0.5000\n",
            "  Epoch 10/20 - Train Acc: 0.7324 - Val Acc: 0.8046\n",
            "  Epoch 15/20 - Train Acc: 0.8138 - Val Acc: 0.8488\n",
            "  Epoch 20/20 - Train Acc: 0.8365 - Val Acc: 0.8594\n",
            "  Final Test Acc: 0.8576, Best Val Acc: 0.8594\n",
            "\n",
            "==================================================\n",
            "ğŸš€ STARTING EXPERIMENT 3/5\n",
            "Parameters: {'LR': 0.001, 'DROPOUT': 0.5, 'EMBEDDING': 128, 'MARGIN': 1.0}\n",
            "==================================================\n",
            "  Epoch 1/20 - Train Acc: 0.7476 - Val Acc: 0.8937\n",
            "  Epoch 5/20 - Train Acc: 0.9236 - Val Acc: 0.9484\n",
            "  Epoch 10/20 - Train Acc: 0.9358 - Val Acc: 0.9567\n",
            "  Epoch 15/20 - Train Acc: 0.9431 - Val Acc: 0.9599\n",
            "  Epoch 20/20 - Train Acc: 0.9505 - Val Acc: 0.9655\n",
            "  Final Test Acc: 0.9623, Best Val Acc: 0.9669\n",
            "\n",
            "==================================================\n",
            "ğŸš€ STARTING EXPERIMENT 4/5\n",
            "Parameters: {'LR': 0.001, 'DROPOUT': 0.3, 'EMBEDDING': 64, 'MARGIN': 1.0}\n",
            "==================================================\n",
            "  Epoch 1/20 - Train Acc: 0.5111 - Val Acc: 0.5645\n",
            "  Epoch 5/20 - Train Acc: 0.5001 - Val Acc: 0.5000\n",
            "  Epoch 10/20 - Train Acc: 0.5000 - Val Acc: 0.5000\n",
            "  Epoch 15/20 - Train Acc: 0.5000 - Val Acc: 0.5000\n",
            "  Epoch 20/20 - Train Acc: 0.5001 - Val Acc: 0.5000\n",
            "  Final Test Acc: 0.5000, Best Val Acc: 0.7990\n",
            "\n",
            "==================================================\n",
            "ğŸš€ STARTING EXPERIMENT 5/5\n",
            "Parameters: {'LR': 0.001, 'DROPOUT': 0.3, 'EMBEDDING': 128, 'MARGIN': 1.5}\n",
            "==================================================\n",
            "  Epoch 1/20 - Train Acc: 0.5423 - Val Acc: 0.7043\n",
            "  Epoch 5/20 - Train Acc: 0.8311 - Val Acc: 0.8844\n",
            "  Epoch 10/20 - Train Acc: 0.5000 - Val Acc: 0.5000\n",
            "  Epoch 15/20 - Train Acc: 0.5000 - Val Acc: 0.5000\n",
            "  Epoch 20/20 - Train Acc: 0.5000 - Val Acc: 0.5000\n",
            "  Final Test Acc: 0.5000, Best Val Acc: 0.8844\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸ FINAL EXPERIMENT RESULTS ğŸ\n",
            "================================================================================\n",
            "| # | LR     | Dropout | Embedding | Margin | Test Acc | Test Loss | Best Val Acc |\n",
            "-----------------------------------------------------------------------------------\n",
            "| 1 | 0.001  | 0.5     | 128       | 1.0    |  96.23%   |    0.0371 |      96.69%   |\n",
            "| 2 | 0.0001 | 0.3     | 128       | 1.0    |  85.76%   |    0.1114 |      85.94%   |\n",
            "| 3 | 0.001  | 0.3     | 128       | 1.0    |  50.00%   |       nan |      88.94%   |\n",
            "| 4 | 0.001  | 0.3     | 64        | 1.0    |  50.00%   |       nan |      79.90%   |\n",
            "| 5 | 0.001  | 0.3     | 128       | 1.5    |  50.00%   |       nan |      88.44%   |\n",
            "-----------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "##  ØªØ­Ù„ÛŒÙ„ Ú©ÙˆØªØ§Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¢Ø²Ù…Ø§ÛŒØ´\n",
        "\n",
        "* **Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø¨Ø±Ù†Ø¯Ù‡ (Ø±ØªØ¨Ù‡ Û±):**\n",
        "    * **Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:** `LR=0.001`, `Dropout=0.5`, `Embedding=128`\n",
        "    * **Ù†ØªÛŒØ¬Ù‡:** **Û¹Û¶.Û²Û³Ùª** Ø¯Ù‚Øª ØªØ³Øª.\n",
        "    * **ØªØ­Ù„ÛŒÙ„:** Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ù‡ØªØ±ÛŒÙ† ØªØ¹Ø§Ø¯Ù„ Ø±Ø§ Ø¯Ø§Ø´Øª. `Dropout=0.5` (ØªÙ†Ø¸ÛŒÙ…â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ù‚ÙˆÛŒ) ØªÙˆØ§Ù†Ø³Øª Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‡Ø§Ø¬Ù…ÛŒ `LR=0.001` Ø±Ø§ Ù…Ù‡Ø§Ø± Ùˆ Ù¾Ø§ÛŒØ¯Ø§Ø± Ú©Ù†Ø¯ Ùˆ Ù…Ø§Ù†Ø¹ Ø§Ø² Ø§Ù†ÙØ¬Ø§Ø± Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ø´ÙˆØ¯.\n",
        "\n",
        "* **Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø´Ú©Ø³Øªâ€ŒØ®ÙˆØ±Ø¯Ù‡ (Ø±ØªØ¨Ù‡â€ŒÙ‡Ø§ÛŒ Û³ØŒ Û´ØŒ Ûµ):**\n",
        "    * **Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:** Ù‡Ù…Ú¯ÛŒ `LR=0.001` Ùˆ `Dropout=0.3` (ÛŒØ§ Ø¨Ø¯ØªØ±).\n",
        "    * **Ù†ØªÛŒØ¬Ù‡:** **ÛµÛ°.Û°Û°Ùª** Ø¯Ù‚Øª (Ù…Ø¹Ø§Ø¯Ù„ Ø´Ø§Ù†Ø³) Ø¨Ø§ `Test Loss = nan`.\n",
        "    * **ØªØ­Ù„ÛŒÙ„:** Ø§ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¯Ú†Ø§Ø± **Ø§Ù†ÙØ¬Ø§Ø± Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† (Exploding Gradient)** Ø´Ø¯Ù†Ø¯. ØªØ±Ú©ÛŒØ¨ Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ø§Ù„Ø§ (`0.001`) Ø¨Ø§ Dropout Ù†Ø§Ú©Ø§ÙÛŒ (`0.3`) Ø³Ù…ÛŒ Ø¨ÙˆØ¯ Ùˆ Ø¨Ø§Ø¹Ø« ÙØ±ÙˆÙ¾Ø§Ø´ÛŒ Ú©Ø§Ù…Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø´Ø¯.\n",
        "\n",
        "* **Ù…Ø¯Ù„ Ú©Ù†Ø¯ (Ø±ØªØ¨Ù‡ Û²):**\n",
        "    * **Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:** `LR=0.0001`, `Dropout=0.3`\n",
        "    * **Ù†ØªÛŒØ¬Ù‡:** **Û¸Ûµ.Û·Û¶Ùª** Ø¯Ù‚Øª ØªØ³Øª.\n",
        "    * **ØªØ­Ù„ÛŒÙ„:** Ø§ÛŒÙ† Ù…Ø¯Ù„ Ù¾Ø§ÛŒØ¯Ø§Ø± Ø¨ÙˆØ¯ Ø§Ù…Ø§ Ø¨Ù‡ Ø´Ø¯Øª **Ú©Ù…â€ŒØ¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ (Under-trained)** Ø§Ø³Øª. Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ø³ÛŒØ§Ø± Ù¾Ø§ÛŒÛŒÙ† Ø¢Ù† Ø¨Ø§Ø¹Ø« Ø´Ø¯ Ø¯Ø± Û²Û° Epoch Ø¨Ù‡ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø®ÙˆØ¯ Ù†Ø±Ø³Ø¯.\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "iSQtTvx4TydU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DMI-o7b-UW6Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}